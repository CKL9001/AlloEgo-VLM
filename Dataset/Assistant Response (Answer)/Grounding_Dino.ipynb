{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430c2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"/root/kl/unsloth_vlm_full_dataset_4o/gpt-4o_2.csv\"\n",
    "data = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "count = 0\n",
    "reference_object_name = data[\"Answers\"][count].split(\"Reference Object: \")[1].split(\"\\n\")[0].strip()\n",
    "target_object_name = data[\"Answers\"][count].split(\"Target Object: \")[1].split(\"\\n\")[0].strip()\n",
    "image_path_name = data[\"image_paths\"][count].strip()\n",
    "\n",
    "CONFIG_PATH = \"groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "CHECKPOINT_PATH = \"./groundingdino_swint_ogc.pth\"\n",
    "DEVICE = \"cpu\"\n",
    "IMAGE_PATH = image_path_name\n",
    "TEXT_PROMPT_Ref_Obj = reference_object_name\n",
    "TEXT_PROMPT_Tar_Obj = target_object_name\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "model = load_model(CONFIG_PATH, CHECKPOINT_PATH)\n",
    "\n",
    "Ref_Obj_Bbox, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT_Ref_Obj,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "Ref_Obj_Bbox_list = Ref_Obj_Bbox.tolist()[0]\n",
    "\n",
    "Tar_Obj_Bbox, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT_Tar_Obj,\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "Tar_Obj_Bbox_list = Tar_Obj_Bbox.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6d6395e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3368787169456482,\n",
       " 0.6371487975120544,\n",
       " 0.0827253982424736,\n",
       " 0.4411730468273163]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ref_Obj_Bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b674727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5124080777168274,\n",
       " 0.6462185382843018,\n",
       " 0.10676079988479614,\n",
       " 0.4224409759044647]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tar_Obj_Bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f60f04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Object:  Person in red\n",
      "Target Object:  Person in gray\n",
      "Image Path:  /root/kl/unsloth_vlm/gpt-4o-mini/1.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Reference Object: \", reference_object_name)\n",
    "print(\"Target Object: \", target_object_name)\n",
    "print(\"Image Path: \", image_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "494a2ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Object - Person in red: 中間偏左下方\n",
      "Target Object - Person in gray: 中間偏下方\n"
     ]
    }
   ],
   "source": [
    "def get_grid_indices(image_shape, bbox_cxcywh):\n",
    "    H, W = image_shape\n",
    "    cx, cy, bw, bh = bbox_cxcywh\n",
    "    x1 = (cx - bw / 2) * W\n",
    "    y1 = (cy - bh / 2) * H\n",
    "    x2 = (cx + bw / 2) * W\n",
    "    y2 = (cy + bh / 2) * H\n",
    "    cell_w = W / 3\n",
    "    cell_h = H / 3\n",
    "    grid_indices = []\n",
    "    for row in range(3):         \n",
    "        for col in range(3):     \n",
    "            grid_id = row * 3 + col + 1\n",
    "            gx1 = int(col * cell_w)\n",
    "            gy1 = int(row * cell_h)\n",
    "            gx2 = int((col + 1) * cell_w)\n",
    "            gy2 = int((row + 1) * cell_h)\n",
    "            if not (x2 <= gx1 or x1 >= gx2 or y2 <= gy1 or y1 >= gy2):\n",
    "                grid_indices.append(grid_id)\n",
    "    return sorted(grid_indices)\n",
    "\n",
    "def grid_to_description(grids):\n",
    "    # 1 | 2 | 3\n",
    "    # ---------\n",
    "    # 4 | 5 | 6\n",
    "    # ---------\n",
    "    # 7 | 8 | 9\n",
    "    if len(grids) == 0:\n",
    "        return \"位置不明\"\n",
    "    elif len(grids) == 1:\n",
    "        if grids ==[1]:\n",
    "            return \"左上方\"\n",
    "        elif grids == [2]:\n",
    "            return \"上方\"\n",
    "        elif grids == [3]:\n",
    "            return \"右上方\"\n",
    "        elif grids == [4]:\n",
    "            return \"左側\"\n",
    "        elif grids == [5]:\n",
    "            return \"中央\"\n",
    "        elif grids == [6]:\n",
    "            return \"右側\"\n",
    "        elif grids == [7]:\n",
    "            return \"左下方\"\n",
    "        elif grids == [8]:\n",
    "            return \"下方\"\n",
    "        elif grids == [9]:  \n",
    "            return \"右下方\"\n",
    "    elif len(grids) == 2:\n",
    "        if grids == [1,2] or grids == [1,4]:\n",
    "            return \"左上方\"\n",
    "        elif grids == [2,3] or grids == [3,6]:\n",
    "            return \"右上方\"\n",
    "        elif grids == [4,7] or grids == [7,8]:\n",
    "            return \"左下方\"\n",
    "        elif grids == [6,9] or grids == [8,9]:\n",
    "            return \"右下方\"\n",
    "        elif grids == [2,5] :\n",
    "            return \"中間偏上方\"\n",
    "        elif grids == [4,5]:\n",
    "            return \"中間偏左方\"\n",
    "        elif grids == [5,6]:\n",
    "            return \"中間偏右方\"\n",
    "        elif grids == [5,8]:\n",
    "            return \"中間偏下方\"\n",
    "    elif len(grids) == 3:\n",
    "        if grids == [1,2,3]:\n",
    "            return \"上方\"\n",
    "        elif grids == [4,5,6] or grids == [2,5,8]:\n",
    "            return \"中間\"\n",
    "        elif grids == [7,8,9]:\n",
    "            return \"下方\"\n",
    "        elif grids == [1,4,7]:\n",
    "            return \"左側\"\n",
    "        elif grids == [3,6,9]:\n",
    "            return \"右側\"\n",
    "    elif len(grids) == 4:\n",
    "        if grids == [1,2,4,5]:\n",
    "            return \"中間偏左上方\"\n",
    "        elif grids == [2,3,5,6]:\n",
    "            return \"中間偏右上方\"\n",
    "        elif grids == [4,5,7,8]:\n",
    "            return \"中間偏左下方\"\n",
    "        elif grids == [5,6,8,9]:\n",
    "            return \"中間偏右下方\"\n",
    "    elif len(grids) == 6:\n",
    "        if grids == [1,2,3,4,5,6]:\n",
    "            return \"中間偏上方\"\n",
    "        elif grids == [4,5,6,7,8,9]:\n",
    "            return \"中間偏下方\"\n",
    "        elif grids == [1,2,4,5,7,8]:\n",
    "            return \"中間偏左側\"\n",
    "        elif grids == [2,3,5,6,8,9]:\n",
    "            return \"中間偏右側\"\n",
    "    elif len(grids) == 9:\n",
    "        return \"中間\"\n",
    "\n",
    "ref_grids = get_grid_indices(image.shape[:2], Ref_Obj_Bbox_list)\n",
    "tar_grids = get_grid_indices(image.shape[:2], Tar_Obj_Bbox_list)\n",
    "\n",
    "print(\"Reference Object - \"+reference_object_name+\": \"+grid_to_description(ref_grids))\n",
    "print(\"Target Object - \"+target_object_name+\": \"+grid_to_description(tar_grids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97401c77",
   "metadata": {},
   "source": [
    "# Position Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55115e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_algorithm():\n",
    "    ref_grids = get_grid_indices(image.shape[:2], Ref_Obj_Bbox_list)\n",
    "    tar_grids = get_grid_indices(image.shape[:2], Tar_Obj_Bbox_list)\n",
    "    reference_object_position = grid_to_description(ref_grids)\n",
    "    target_object_position = grid_to_description(tar_grids)\n",
    "    return reference_object_position, target_object_position\n",
    "    \n",
    "\n",
    "# def direction_algorithm(image_shape, bbox_cxcywh):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420d480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('中間偏左下方', '中間偏下方')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_algorithm() # reference_object_position, target_object_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ad183",
   "metadata": {},
   "source": [
    "# SAM + Direction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07009835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/kl/unsloth_vlm/gpt-4o-mini/1.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_name #'/root/kl/unsloth_vlm/gpt-4o-mini/1.jpg'\n",
    "Depth_map = image_path_name.replace('.jpg', '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Ultralytics settings reset to default values. This may be due to a possible problem with your settings or a recent ultralytics package update. \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/sam_b.pt to 'sam_b.pt': 100% ━━━━━━━━━━━━ 357.7/357.7MB 1.3MB/s 4:32309s\n",
      "\n",
      "0: 1024x1024 1 0, 1193.3ms\n",
      "Speed: 96.9ms preprocess, 1193.3ms inference, 63.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 97.9ms\n",
      "Speed: 5.1ms preprocess, 97.9ms inference, 0.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Ref mask shape: (600, 800) unique: [0 1]\n",
      "Tar mask shape: (600, 800) unique: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "from ultralytics import SAM\n",
    "\n",
    "image = Image.open(image_path_name).convert('RGB')\n",
    "image_numpy = np.array(image)\n",
    "\n",
    "sam = SAM(\"sam_b.pt\")  \n",
    "\n",
    "h, w, _ = image_numpy.shape\n",
    "\n",
    "def convert_bbox(bbox, w, h):\n",
    "    cx, cy, bw, bh = bbox\n",
    "    x_min = int((cx - bw/2) * w)\n",
    "    y_min = int((cy - bh/2) * h)\n",
    "    x_max = int((cx + bw/2) * w)\n",
    "    y_max = int((cy + bh/2) * h)\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "ref_box = convert_bbox(Ref_Obj_Bbox_list, w, h)\n",
    "tar_box = convert_bbox(Tar_Obj_Bbox_list, w, h)\n",
    "\n",
    "ref_results = sam.predict(image_numpy, bboxes=[ref_box])\n",
    "tar_results = sam.predict(image_numpy, bboxes=[tar_box])\n",
    "\n",
    "ref_mask = (ref_results[0].masks.data[0].cpu().numpy() * 1).astype(np.uint8)\n",
    "tar_mask = (tar_results[0].masks.data[0].cpu().numpy() * 1).astype(np.uint8)\n",
    "\n",
    "print(\"Ref mask shape:\", ref_mask.shape, \"unique:\", np.unique(ref_mask))\n",
    "print(\"Tar mask shape:\", tar_mask.shape, \"unique:\", np.unique(tar_mask))\n",
    "\n",
    "Depth_map = pd.read_csv(image_path_name.replace('.jpg', '.csv'))\n",
    "\n",
    "ref_mask_depth = Depth_map * ref_mask\n",
    "tar_mask_depth = Depth_map * tar_mask\n",
    "\n",
    "ref_mask_depth_mean = ref_mask_depth.mean()\n",
    "tar_mask_depth_mean = tar_mask_depth.mean()\n",
    "\n",
    "thereshold = 50 #cm\n",
    "if ref_mask_depth_mean + thereshold < tar_mask_depth_mean:\n",
    "    print(\"Target object is behind the reference object.\")\n",
    "\n",
    "if tar_mask_depth_mean + thereshold < ref_mask_depth_mean:\n",
    "    print(\"Target object is in front of the reference object.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
